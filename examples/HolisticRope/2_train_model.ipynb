{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seonyu\\project\\PipeRun\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\seonyu\\\\project\\\\PipeRun'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1207, 30, 173)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [\n",
    "    'rope',\n",
    "    'stop'\n",
    "]\n",
    "\n",
    "data = np.concatenate([\n",
    "    np.load('dataset/seq_rope_1634032360.npy'),\n",
    "    np.load('dataset/seq_stop_1634032360.npy'),\n",
    "], axis=0)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1207, 30, 172)\n",
      "(1207,)\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0.]), array([0., 0., 0., ..., 1., 1., 1.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:5], labels[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1207, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1086, 30, 172) (1086, 2)\n",
      "(121, 30, 172) (121, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=42)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 64)                60672     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 62,818\n",
      "Trainable params: 62,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "34/34 [==============================] - 1s 16ms/step - loss: 0.1928 - acc: 0.9088 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to models\\airrope_modelss.h5\n",
      "Epoch 2/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 1.6826e-07 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 1.00000\n",
      "Epoch 3/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 1.00000\n",
      "Epoch 4/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 1.00000\n",
      "Epoch 5/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 1.00000\n",
      "Epoch 6/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 1.00000\n",
      "Epoch 7/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 1.00000\n",
      "Epoch 8/150\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 1.00000\n",
      "Epoch 9/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 1.00000\n",
      "Epoch 10/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 1.00000\n",
      "Epoch 11/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 1.00000\n",
      "Epoch 12/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 1.00000\n",
      "Epoch 13/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 1.00000\n",
      "Epoch 14/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 1.00000\n",
      "Epoch 15/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 1.00000\n",
      "Epoch 16/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 1.00000\n",
      "Epoch 17/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 1.00000\n",
      "Epoch 18/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 1.00000\n",
      "Epoch 19/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 1.00000\n",
      "Epoch 20/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 1.00000\n",
      "Epoch 21/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 1.00000\n",
      "Epoch 22/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 1.00000\n",
      "Epoch 23/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 1.00000\n",
      "Epoch 24/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 1.00000\n",
      "Epoch 25/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 1.00000\n",
      "Epoch 26/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 1.00000\n",
      "Epoch 27/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 1.00000\n",
      "Epoch 28/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 1.00000\n",
      "Epoch 29/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 1.00000\n",
      "Epoch 30/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 1.00000\n",
      "Epoch 31/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 1.00000\n",
      "Epoch 32/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 1.00000\n",
      "Epoch 33/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 1.00000\n",
      "Epoch 34/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 1.00000\n",
      "Epoch 35/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 1.00000\n",
      "Epoch 36/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 1.00000\n",
      "Epoch 37/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 1.00000\n",
      "Epoch 38/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 1.00000\n",
      "Epoch 39/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 1.00000\n",
      "Epoch 40/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 1.00000\n",
      "Epoch 41/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 1.00000\n",
      "Epoch 42/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 1.00000\n",
      "Epoch 43/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 1.00000\n",
      "Epoch 44/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 1.00000\n",
      "Epoch 45/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 1.00000\n",
      "Epoch 46/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 1.00000\n",
      "Epoch 47/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 1.00000\n",
      "Epoch 48/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 1.00000\n",
      "Epoch 49/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 1.00000\n",
      "Epoch 50/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 1.00000\n",
      "Epoch 51/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 52/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 1.00000\n",
      "Epoch 53/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 1.00000\n",
      "Epoch 54/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 1.00000\n",
      "Epoch 55/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 1.00000\n",
      "Epoch 56/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 1.00000\n",
      "Epoch 57/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 1.00000\n",
      "Epoch 58/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 1.00000\n",
      "Epoch 59/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 1.00000\n",
      "Epoch 60/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 1.00000\n",
      "Epoch 61/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 1.00000\n",
      "Epoch 62/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 1.00000\n",
      "Epoch 63/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 1.00000\n",
      "Epoch 64/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 1.00000\n",
      "Epoch 65/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 1.00000\n",
      "Epoch 66/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 1.00000\n",
      "Epoch 67/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 1.00000\n",
      "Epoch 68/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 1.00000\n",
      "Epoch 69/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 1.00000\n",
      "Epoch 70/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 1.00000\n",
      "Epoch 71/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 1.00000\n",
      "Epoch 72/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 1.00000\n",
      "Epoch 73/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 1.00000\n",
      "Epoch 74/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 1.00000\n",
      "Epoch 75/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 1.00000\n",
      "Epoch 76/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 1.00000\n",
      "Epoch 77/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 1.00000\n",
      "Epoch 78/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 1.00000\n",
      "Epoch 79/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 1.00000\n",
      "Epoch 80/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 1.00000\n",
      "Epoch 81/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 1.00000\n",
      "Epoch 82/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 1.00000\n",
      "Epoch 83/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 1.00000\n",
      "Epoch 84/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 1.00000\n",
      "Epoch 85/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 1.00000\n",
      "Epoch 86/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 1.00000\n",
      "Epoch 87/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 1.00000\n",
      "Epoch 88/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 1.00000\n",
      "Epoch 89/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 1.00000\n",
      "Epoch 90/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 1.00000\n",
      "Epoch 91/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 1.00000\n",
      "Epoch 92/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 1.00000\n",
      "Epoch 93/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 1.00000\n",
      "Epoch 94/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 1.00000\n",
      "Epoch 95/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 1.00000\n",
      "Epoch 96/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 1.00000\n",
      "Epoch 97/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 1.00000\n",
      "Epoch 98/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 1.00000\n",
      "Epoch 99/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 1.00000\n",
      "Epoch 100/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 1.00000\n",
      "Epoch 101/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 102/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 1.00000\n",
      "Epoch 103/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 1.00000\n",
      "Epoch 104/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 1.00000\n",
      "Epoch 105/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 1.00000\n",
      "Epoch 106/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 1.00000\n",
      "Epoch 107/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 1.00000\n",
      "Epoch 108/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 1.00000\n",
      "Epoch 109/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 1.00000\n",
      "Epoch 110/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 1.00000\n",
      "Epoch 111/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 1.00000\n",
      "Epoch 112/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 1.00000\n",
      "Epoch 113/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 1.00000\n",
      "Epoch 114/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 1.00000\n",
      "Epoch 115/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 1.00000\n",
      "Epoch 116/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 1.00000\n",
      "Epoch 117/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 1.00000\n",
      "Epoch 118/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 1.00000\n",
      "Epoch 119/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 1.00000\n",
      "Epoch 120/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 1.00000\n",
      "Epoch 121/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 1.00000\n",
      "Epoch 122/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 1.00000\n",
      "Epoch 123/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 1.00000\n",
      "Epoch 124/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 1.00000\n",
      "Epoch 125/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 1.00000\n",
      "Epoch 126/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 1.00000\n",
      "Epoch 127/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 1.00000\n",
      "Epoch 128/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 1.00000\n",
      "Epoch 129/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 1.00000\n",
      "Epoch 130/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 1.00000\n",
      "Epoch 131/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 1.00000\n",
      "Epoch 132/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 1.00000\n",
      "Epoch 133/150\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 1.00000\n",
      "Epoch 134/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 1.00000\n",
      "Epoch 135/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 1.00000\n",
      "Epoch 136/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 1.00000\n",
      "Epoch 137/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 1.00000\n",
      "Epoch 138/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 1.00000\n",
      "Epoch 139/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 1.00000\n",
      "Epoch 140/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 1.00000\n",
      "Epoch 141/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 1.00000\n",
      "Epoch 142/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 1.00000\n",
      "Epoch 143/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 1.00000\n",
      "Epoch 144/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 1.00000\n",
      "Epoch 145/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 1.00000\n",
      "Epoch 146/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 1.00000\n",
      "Epoch 147/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 1.00000\n",
      "Epoch 148/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 1.00000\n",
      "Epoch 149/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 1.00000\n",
      "Epoch 150/150\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 1.00000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=150,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/airrope_modelss.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[50,  0],\n",
       "        [ 0, 71]],\n",
       "\n",
       "       [[71,  0],\n",
       "        [ 0, 50]]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/airrope_modelss.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFlite Model Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\seonyu\\AppData\\Local\\Temp\\tmp5oduqkly\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('models/airrope_modelss.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3297a7b6c4f9612d22fa729cafa02454a0b0cc28be99dad307ed9b044052bcf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('piperun': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
